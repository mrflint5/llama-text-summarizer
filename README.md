# ğŸ¦™âœ¨ LLaMA-Powered Text Summarizer ğŸš€  
**Transform raw text into concise, high-quality summaries â€” locally, privately, and at lightning speed.**  

> _Youâ€™re not just summarizing â€” youâ€™re redefining the standard for AI applications with a **fully local, production-grade** architecture._  

---

## ğŸŒŸ Why This Project Stands Out
Most AI summarizers:
- âŒ Send your data to third-party APIs  
- âŒ Depend on costly subscriptions  
- âŒ Lack full-stack transparency  

This project âœ…:
- ğŸ  **Runs 100% Locally** â€” Your data never leaves your machine  
- âš¡ **Performs in Real-Time** â€” Harness your CPU/GPU directly  
- ğŸ›  **Gives Full Control** â€” Swap models instantly via Ollama  
- ğŸ— **Uses Scalable Architecture** â€” Backend (FastAPI) + Frontend (Streamlit) + Local AI Engine  

---

## ğŸ§© Architecture Diagram
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     POST /summarize      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     POST /api/generate      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚   FastAPI     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ Ollama LLaMA â”‚
â”‚  Frontend   â”‚                          â”‚   Backend API â”‚                             â”‚  Local Model â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     JSON Response         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     Model Output            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Tech Stack
- ğŸ¦™ **[Ollama](https://ollama.com/)** â€” Local LLaMA hosting  
- âš¡ **[FastAPI](https://fastapi.tiangolo.com/)** â€” Ultra-fast Python backend  
- ğŸ¨ **[Streamlit](https://streamlit.io/)** â€” Clean, interactive frontend UI  
- ğŸ **Python 3.10+** â€” Modern, powerful programming language  
- ğŸ”— **Requests** â€” Smooth communication between services  

---

## ğŸš€ Quickstart

### 1ï¸âƒ£ Install Ollama & Pull Model
```bash
# Install Ollama (Mac/Linux)
curl -fsSL https://ollama.com/install.sh | sh

# Windows: Download installer from ollama.com

# Pull the LLaMA model
ollama pull llama2
```

### 2ï¸âƒ£ Backend Setup
```bash
git clone https://github.com/yourusername/llama-text-summarizer.git
cd llama-text-summarizer

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run FastAPI backend
uvicorn backend.main:app --reload --port 8000
```

### 3ï¸âƒ£ Frontend Setup
```bash
# From the project root
streamlit run frontend/app.py
```

---

## ğŸ–¥ï¸ Features
- âœ¨ **Instant Summaries** â€” Optimized prompt design  
- ğŸ”’ **Offline Privacy** â€” Zero external API calls  
- ğŸ”„ **Model Flexibility** â€” Choose any supported Ollama model  
- âš¡ **Parallel Performance** â€” Independent backend/frontend runtime  
- ğŸ“œ **User-Friendly UI** â€” Minimal yet powerful  

---

## ğŸ§ª Example
**Input:**
```
The global economy is facing unprecedented challenges, with inflation...
```
**Output:**
```
The global economy faces inflationary pressures, supply chain issues, and energy market shifts, prompting urgent policy responses.
```

---

## ğŸ“‚ Project Structure
```
.
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ main.py          # FastAPI backend API
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py           # Streamlit frontend
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”¥ Advanced Tips
- ğŸª„ **Change Models**:
```bash
ollama pull mistral
```
Update `"model": "mistral"` in requests  
- ğŸ³ **Dockerize** for deployment  
- ğŸ” **Add Auth** in FastAPI for private access  

---

## ğŸ“¸ Working Screenshot
[ğŸ“‚ View Screenshot Folder](https://drive.google.com/drive/folders/1OigJFAN2Tpw0qWdF94RRP4H2bpuH23ef?usp=sharing)

---

## ğŸ¤ Connect
ğŸ“§ **Email:** [sameermalik1419@gmail.com](mailto:sameermalik1419@gmail.com)  
ğŸ’¼ **LinkedIn:** [Sameer Malik](https://www.linkedin.com/in/sameer-malik-b5b8772b9)  
â­ If you love this project, **give it a star**!
